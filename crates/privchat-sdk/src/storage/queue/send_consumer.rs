use crate::storage::queue::TaskQueueTrait;
use crate::storage::queue::send_task::SendTask;
use crate::storage::queue::retry_policy::{RetryManager, SendFailureReason};
use crate::storage::StorageManager;
use crate::network::{NetworkMonitor, NetworkStatus, NetworkStatusEvent};
use crate::client::PrivchatClient;
use crate::error::{PrivchatSDKError, Result};
use crate::rate_limiter::MessageRateLimiter;
use crate::events::{EventManager, SDKEvent, SendStatusState};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use tokio::sync::{Mutex, RwLock};
use tokio::time::{sleep, timeout, Instant as TokioInstant};
use tokio_util::time::DelayQueue;
use tokio::select;
use futures::StreamExt;
use tracing::{debug, error, info, warn, instrument};

/// æ¶ˆæ¯å‘é€ç»“æœï¼ˆä»…ç”¨äºæ¶ˆæ¯é˜Ÿåˆ—å†…éƒ¨ï¼‰
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SendResult {
    /// æœåŠ¡ç«¯æ¶ˆæ¯IDï¼ˆå‘é€æˆåŠŸæ—¶æ‰æœ‰å€¼ï¼‰
    pub server_msg_id: Option<u64>,
    /// å‘é€æ—¶é—´æˆ³
    pub sent_at: u64,
    /// æ˜¯å¦æˆåŠŸ
    pub success: bool,
    /// é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰
    pub error_message: Option<String>,
}

/// é¢‘é“é”®ï¼Œç”¨äºæ§åˆ¶ä¸²è¡Œå‘é€
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub struct ChannelKey {
    pub channel_id: u64,  // u64ï¼Œä¸æœåŠ¡ç«¯ä¸€è‡´
    pub channel_type: i32,
}

impl ChannelKey {
    pub fn new(channel_id: u64, channel_type: i32) -> Self {
        Self { channel_id, channel_type }
    }
    
    pub fn from_task(task: &SendTask) -> Self {
        Self::new(
            task.message_data.channel_id,  // u64ï¼Œç›´æ¥ä½¿ç”¨
            task.message_data.channel_type,
        )
    }
}

/// å‘é€æ¶ˆè´¹è€…é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SendConsumerConfig {
    /// å·¥ä½œçº¿ç¨‹æ•°é‡
    pub worker_count: usize,
    /// æ‰¹é‡æ‹‰å–ä»»åŠ¡æ•°é‡
    pub batch_size: usize,
    /// æ‹‰å–ä»»åŠ¡é—´éš”ï¼ˆæ¯«ç§’ï¼‰
    pub poll_interval_ms: u64,
    /// å‘é€è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pub send_timeout_seconds: u64,
    /// æœ€å¤§å¹¶å‘é¢‘é“æ•°
    pub max_concurrent_channels: usize,
    /// å»¶è¿Ÿé˜Ÿåˆ—å®¹é‡
    pub delay_queue_capacity: usize,
}

impl Default for SendConsumerConfig {
    fn default() -> Self {
        Self {
            worker_count: 4,
            batch_size: 10,
            poll_interval_ms: 100,
            send_timeout_seconds: 30,
            max_concurrent_channels: 100,
            delay_queue_capacity: 1000,
        }
    }
}

/// å‘é€ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Default)]
pub struct SendMetrics {
    pub send_attempt_total: u64,
    pub send_success_total: u64,
    pub send_failure_total: u64,
    pub retry_count_total: u64,
    pub channel_serial_violations: u64,
    pub average_retry_count: f64,
}

impl SendMetrics {
    pub fn success_rate(&self) -> f64 {
        if self.send_attempt_total == 0 {
            0.0
        } else {
            self.send_success_total as f64 / self.send_attempt_total as f64
        }
    }
    
    pub fn update_retry_average(&mut self) {
        if self.send_attempt_total > 0 {
            self.average_retry_count = self.retry_count_total as f64 / self.send_attempt_total as f64;
        }
    }
}

/// å»¶è¿Ÿé‡è¯•ä»»åŠ¡
#[derive(Debug)]
#[allow(dead_code)]
struct DelayedTask {
    task: SendTask,
    retry_at: TokioInstant,
}

/// å‘é€æ¶ˆè´¹è€…è¿è¡Œå™¨
pub struct SendConsumerRunner {
    config: SendConsumerConfig,
    task_queue: Arc<dyn TaskQueueTrait>,
    storage_manager: Arc<StorageManager>,
    client: Arc<tokio::sync::RwLock<Option<PrivchatClient>>>,
    network_monitor: Arc<NetworkMonitor>,
    retry_manager: Arc<RetryManager>,
    
    // é¢‘é“çº§ä¸²è¡Œæ§åˆ¶
    channel_locks: Arc<RwLock<HashMap<ChannelKey, Arc<Mutex<()>>>>>,
    
    // å»¶è¿Ÿé‡è¯•é˜Ÿåˆ—
    delay_queue: Arc<Mutex<DelayQueue<DelayedTask>>>,
    
    // ç»Ÿè®¡ä¿¡æ¯
    metrics: Arc<RwLock<SendMetrics>>,
    
    // æ§åˆ¶ä¿¡å·
    shutdown_signal: Arc<tokio::sync::Notify>,
    is_running: Arc<RwLock<bool>>,
    
    // æ¶ˆæ¯å‘é€é™æµå™¨
    message_rate_limiter: Arc<MessageRateLimiter>,
    
    // äº‹ä»¶ç®¡ç†å™¨ï¼ˆç”¨äºå‘é€çŠ¶æ€é€šçŸ¥ï¼‰
    event_manager: Arc<EventManager>,
}

impl SendConsumerRunner {
    pub fn new(
        config: SendConsumerConfig,
        task_queue: Arc<dyn TaskQueueTrait>,
        storage_manager: Arc<StorageManager>,
        client: Arc<tokio::sync::RwLock<Option<PrivchatClient>>>,
        network_monitor: Arc<NetworkMonitor>,
        retry_manager: Arc<RetryManager>,
        message_rate_limiter: Arc<MessageRateLimiter>,
        event_manager: Arc<EventManager>,
    ) -> Self {
        Self {
            config,
            task_queue,
            storage_manager,
            client,
            network_monitor,
            retry_manager,
            channel_locks: Arc::new(RwLock::new(HashMap::new())),
            delay_queue: Arc::new(Mutex::new(DelayQueue::with_capacity(1000))),
            metrics: Arc::new(RwLock::new(SendMetrics::default())),
            shutdown_signal: Arc::new(tokio::sync::Notify::new()),
            is_running: Arc::new(RwLock::new(false)),
            message_rate_limiter,
            event_manager,
        }
    }
    

    /// å¯åŠ¨æ¶ˆè´¹è€…
    #[instrument(skip(self))]
    pub async fn start(&self) -> Result<()> {
        {
            let mut running = self.is_running.write().await;
            if *running {
                return Err(PrivchatSDKError::Other("Consumer already running".to_string()));
            }
            *running = true;
        }

        info!("Starting SendConsumer with {} workers", self.config.worker_count);

        // å¯åŠ¨ç½‘ç»œçŠ¶æ€ç›‘å¬
        let mut network_events = self.network_monitor.subscribe();
        let shutdown_clone = self.shutdown_signal.clone();
        tokio::spawn(async move {
            loop {
                select! {
                    event = network_events.recv() => {
                        match event {
                            Ok(NetworkStatusEvent { new_status, .. }) => {
                                info!("Network status changed to: {:?}", new_status);
                            }
                            Err(_) => break,
                        }
                    }
                    _ = shutdown_clone.notified() => break,
                }
            }
        });

        // å¯åŠ¨å»¶è¿Ÿé˜Ÿåˆ—å¤„ç†å™¨
        self.start_delay_queue_processor().await;

        // å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for worker_id in 0..self.config.worker_count {
            self.start_worker(worker_id).await;
        }

        Ok(())
    }

    /// åœæ­¢æ¶ˆè´¹è€…
    #[instrument(skip(self))]
    pub async fn stop(&self) -> Result<()> {
        info!("Stopping SendConsumer");
        
        {
            let mut running = self.is_running.write().await;
            *running = false;
        }

        self.shutdown_signal.notify_waiters();
        
        // ç­‰å¾…ä¸€æ®µæ—¶é—´è®©å·¥ä½œçº¿ç¨‹ä¼˜é›…é€€å‡º
        sleep(Duration::from_millis(500)).await;
        
        info!("SendConsumer stopped");
        Ok(())
    }

    /// å¯åŠ¨å·¥ä½œçº¿ç¨‹
    async fn start_worker(&self, worker_id: usize) {
        let task_queue = self.task_queue.clone();
        let storage_manager = self.storage_manager.clone();
        let client = self.client.clone();
        let network_monitor = self.network_monitor.clone();
        let retry_manager = self.retry_manager.clone();
        let channel_locks = self.channel_locks.clone();
        let delay_queue = self.delay_queue.clone();
        let metrics = self.metrics.clone();
        let shutdown_signal = self.shutdown_signal.clone();
        let is_running = self.is_running.clone();
        let config = self.config.clone();
        let message_rate_limiter = self.message_rate_limiter.clone();
        let event_manager = self.event_manager.clone();

        tokio::spawn(async move {
            info!("Worker {} started", worker_id);

            loop {
                select! {
                    _ = shutdown_signal.notified() => {
                        info!("Worker {} received shutdown signal", worker_id);
                        break;
                    }
                    _ = sleep(Duration::from_millis(config.poll_interval_ms)) => {
                        if !*is_running.read().await {
                            break;
                        }

                        // æ£€æŸ¥ç½‘ç»œçŠ¶æ€
                        let network_status = network_monitor.get_status().await;
                        if network_status != NetworkStatus::Online {
                            info!("Worker {} skipping due to network status: {:?}", worker_id, network_status);
                            continue;
                        }

                        info!("Worker {} attempting to pull tasks...", worker_id);

                        // æ‹‰å–ä»»åŠ¡
                        match Self::pull_and_process_tasks(
                            worker_id,
                            &*task_queue,
                            &*storage_manager,
                            &client,
                            &*retry_manager,
                            &channel_locks,
                            &delay_queue,
                            &metrics,
                            &config,
                            &*message_rate_limiter,
                            &*event_manager,
                        ).await {
                            Ok(processed_count) => {
                                if processed_count > 0 {
                                    debug!("Worker {} processed {} tasks", worker_id, processed_count);
                                }
                            }
                            Err(e) => {
                                error!("Worker {} error: {}", worker_id, e);
                            }
                        }
                    }
                }
            }

            info!("Worker {} stopped", worker_id);
        });
    }

    /// æ‹‰å–å¹¶å¤„ç†ä»»åŠ¡
    async fn pull_and_process_tasks(
        worker_id: usize,
        task_queue: &dyn TaskQueueTrait,
        storage_manager: &StorageManager,
        client: &Arc<tokio::sync::RwLock<Option<PrivchatClient>>>,
        retry_manager: &RetryManager,
        channel_locks: &Arc<RwLock<HashMap<ChannelKey, Arc<Mutex<()>>>>>,
        delay_queue: &Arc<Mutex<DelayQueue<DelayedTask>>>,
        metrics: &Arc<RwLock<SendMetrics>>,
        config: &SendConsumerConfig,
        message_rate_limiter: &MessageRateLimiter,
        event_manager: &EventManager,
    ) -> Result<usize> {
        // æ‹‰å–ä»»åŠ¡
        let tasks = task_queue.dequeue_batch(config.batch_size).await?;
        if tasks.is_empty() {
            debug!("Worker {} found no tasks in queue", worker_id);
            return Ok(0);
        }
        
        info!("Worker {} pulled {} tasks from queue", worker_id, tasks.len());

        let mut processed_count = 0;

        for mut task in tasks {
            // æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¿‡æœŸ
            if task.is_expired() {
                warn!("Task {} expired, marking as failed", task.id);
                task.mark_expired();
                if let Err(e) = storage_manager.update_message_status(task.id, crate::storage::entities::MessageStatus::Failed as i32).await {
                    error!("Failed to update expired message status: {}", e);
                }
                continue;
            }

            // è·å–é¢‘é“é”
            let channel_key = ChannelKey::from_task(&task);
            let channel_lock = Self::get_channel_lock(channel_locks, channel_key).await;
            
            // åœ¨é”ä¿æŠ¤ä¸‹å¤„ç†ä»»åŠ¡
            let _guard = channel_lock.lock().await;
            
            match Self::process_single_task(
                worker_id,
                task,
                storage_manager,
                client,
                retry_manager,
                delay_queue,
                metrics,
                config,
                message_rate_limiter,
                event_manager,
            ).await {
                Ok(_) => processed_count += 1,
                Err(e) => {
                    error!("Worker {} failed to process task: {}", worker_id, e);
                }
            }
        }

        Ok(processed_count)
    }

    /// å¤„ç†å•ä¸ªä»»åŠ¡
    async fn process_single_task(
        worker_id: usize,
        mut task: SendTask,
        storage_manager: &StorageManager,
        client: &Arc<tokio::sync::RwLock<Option<PrivchatClient>>>,
        retry_manager: &RetryManager,
        delay_queue: &Arc<Mutex<DelayQueue<DelayedTask>>>,
        metrics: &Arc<RwLock<SendMetrics>>,
        config: &SendConsumerConfig,
        message_rate_limiter: &MessageRateLimiter,
        event_manager: &EventManager,
    ) -> Result<()> {
        debug!("Worker {} processing task: {}", worker_id, task.id);

        // æ›´æ–°ç»Ÿè®¡
        {
            let mut m = metrics.write().await;
            m.send_attempt_total += 1;
            m.retry_count_total += task.retry_count as u64;
            m.update_retry_average();
        }

        // æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­
        task.mark_processing();
        storage_manager.update_message_status(task.id, crate::storage::entities::MessageStatus::Sending as i32).await?;
        
        // é€šçŸ¥å‘é€çŠ¶æ€ï¼šSending
        let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
        event_manager.emit(SDKEvent::SendStatusUpdate {
            channel_id: task.message_data.channel_id,
            id: task.id as u64,
            state: SendStatusState::Sending,
            attempts: task.retry_count,
            error: None,
            timestamp,
        }).await;

        // ğŸ”¥ æ£€æŸ¥æ¶ˆæ¯å‘é€é™æµ
        let is_group = task.message_data.channel_type != 0; // å‡è®¾ 0 = ç§èŠï¼Œå…¶ä»– = ç¾¤èŠ
        if let Err(wait_duration) = message_rate_limiter.check_send(is_group) {
            debug!(
                "Worker {} æ¶ˆæ¯å‘é€å—é™ï¼ˆ{}ï¼‰ï¼Œç­‰å¾… {}ms",
                worker_id,
                if is_group { "ç¾¤èŠ" } else { "ç§èŠ" },
                wait_duration.as_millis()
            );
            tokio::time::sleep(wait_duration).await;
        }

        // æ‰§è¡Œå‘é€ï¼ˆç›´æ¥è°ƒç”¨ PrivchatClientï¼‰
        let send_result = {
            let mut client_guard = client.write().await;
            let client_ref = client_guard.as_mut()
                .ok_or_else(|| PrivchatSDKError::NotConnected)?;
            
            // æ„é€ æ¶ˆæ¯å†…å®¹ï¼›local_message_id ç”± sdk å…¥é˜Ÿæ—¶é›ªèŠ±ç”Ÿæˆï¼Œå…¼å®¹æ—§ä»»åŠ¡ç”¨ id å›é€€
            let message = &task.message_data;
            let content = &message.content;
            let channel_id = message.channel_id;
            let local_message_id = if message.local_message_id != 0 {
                message.local_message_id
            } else {
                task.id as u64
            };
            
            timeout(
                Duration::from_secs(config.send_timeout_seconds),
                async {
                    // ä» extra ä¸­æå– reply_to_message_idï¼ˆå¦‚æœæœ‰ï¼‰
                    let metadata = if let Some(reply_id_str) = message.extra.get("reply_to_message_id") {
                        if let Ok(reply_id) = reply_id_str.parse::<u64>() {
                            Some(serde_json::json!({
                                "reply_to_message_id": reply_id
                            }))
                        } else {
                            None
                        }
                    } else {
                        None
                    };
                    
                    // ç›´æ¥è°ƒç”¨ client çš„ send_message_internalï¼›local_message_id ç”¨äºæœåŠ¡ç«¯å»é‡
                    let (_ignored, server_message_id) = client_ref.send_message_internal(
                        channel_id,
                        content,
                        "text",
                        metadata,
                        local_message_id,
                    ).await?;
                    
                    // è¿”å› SendResult æ ¼å¼
                    Ok(SendResult {
                        success: true,
                        server_msg_id: Some(server_message_id),
                        sent_at: chrono::Utc::now().timestamp_millis() as u64,
                        error_message: None,
                    })
                }
            ).await
        };

        match send_result {
            Ok(Ok(response)) => {
                // å‘é€æˆåŠŸ
                info!("Worker {} successfully sent message: {} -> {:?}", 
                     worker_id, task.id, response.server_msg_id);
                
                task.mark_completed();
                storage_manager.update_message_status(task.id, crate::storage::entities::MessageStatus::Sent as i32).await?;
                
                // æ›´æ–°æœåŠ¡ç«¯æ¶ˆæ¯IDï¼ˆç”¨äºæ’¤å›ç­‰æ“ä½œï¼‰
                if let Some(server_message_id) = response.server_msg_id {
                    info!("ğŸ” [DEBUG] Worker {} å‡†å¤‡æ›´æ–° message_id: local_message_id={}, server_message_id={}", 
                         worker_id, task.id, server_message_id);
                    
                    if let Err(e) = storage_manager.update_message_server_id(task.id, server_message_id).await {
                        warn!("âŒ Worker {} failed to update server message_id: {}", worker_id, e);
                    } else {
                        info!("âœ… Worker {} æˆåŠŸæ›´æ–° message_id: local_message_id={} -> message_id={}", 
                             worker_id, task.id, server_message_id);
                    }
                } else {
                    warn!("âš ï¸ Worker {} å‘é€æˆåŠŸä½†æœªè¿”å› server_msg_id: local_message_id={}", 
                         worker_id, task.id);
                }
                
                // æ›´æ–°ç»Ÿè®¡
                {
                    let mut m = metrics.write().await;
                    m.send_success_total += 1;
                }
                
                // é€šçŸ¥å‘é€çŠ¶æ€ï¼šSent
                let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
                event_manager.emit(SDKEvent::SendStatusUpdate {
                    channel_id: task.message_data.channel_id,
                    id: task.id as u64,
                    state: SendStatusState::Sent,
                    attempts: task.retry_count,
                    error: None,
                    timestamp,
                }).await;
            }
            Ok(Err(send_error)) => {
                // å‘é€å¤±è´¥ï¼Œå¤„ç†é‡è¯•
                Self::handle_send_failure(
                    worker_id,
                    task,
                    send_error,
                    storage_manager,
                    retry_manager,
                    delay_queue,
                    metrics,
                    event_manager,
                ).await?;
            }
            Err(_) => {
                // è¶…æ—¶
                warn!("Worker {} timeout sending message: {}", worker_id, task.id);
                let timeout_error = PrivchatSDKError::Transport("Send timeout".to_string());
                Self::handle_send_failure(
                    worker_id,
                    task,
                    timeout_error,
                    storage_manager,
                    retry_manager,
                    delay_queue,
                    metrics,
                    event_manager,
                ).await?;
            }
        }

        Ok(())
    }

    /// å¤„ç†å‘é€å¤±è´¥
    async fn handle_send_failure(
        worker_id: usize,
        mut task: SendTask,
        error: PrivchatSDKError,
        storage_manager: &StorageManager,
        retry_manager: &RetryManager,
        delay_queue: &Arc<Mutex<DelayQueue<DelayedTask>>>,
        metrics: &Arc<RwLock<SendMetrics>>,
        event_manager: &EventManager,
    ) -> Result<()> {
        let failure_reason: SendFailureReason = error.into();
        
        warn!("Worker {} send failed for {}: {:?}", 
             worker_id, task.id, failure_reason);

        // æ›´æ–°ç»Ÿè®¡
        {
            let mut m = metrics.write().await;
            m.send_failure_total += 1;
        }

        // æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡è¯•
        match retry_manager.handle_send_failure(task.retry_count, failure_reason.clone())? {
            Some(next_retry_time) => {
                // å¯ä»¥é‡è¯•ï¼ŒåŠ å…¥å»¶è¿Ÿé˜Ÿåˆ—
                task.increment_retry();
                task.next_retry_at = Some(next_retry_time);
                task.mark_failed(format!("{:?}", failure_reason), Some(failure_reason.clone()));
                
                info!("Worker {} scheduling retry for {} at {}", 
                     worker_id, task.id, next_retry_time);
                
                // é€šçŸ¥å‘é€çŠ¶æ€ï¼šRetrying
                let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
                event_manager.emit(SDKEvent::SendStatusUpdate {
                    channel_id: task.message_data.channel_id,
                    id: task.id as u64,
                    state: SendStatusState::Retrying,
                    attempts: task.retry_count,
                    error: Some(format!("{:?}", failure_reason)),
                    timestamp,
                }).await;

                let retry_delay = Duration::from_secs(
                    next_retry_time.saturating_sub(
                        SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs()
                    )
                );

                let delayed_task = DelayedTask {
                    task,
                    retry_at: TokioInstant::now() + retry_delay,
                };

                delay_queue.lock().await.insert(delayed_task, retry_delay);
            }
            None => {
                // ä¸èƒ½é‡è¯•ï¼Œæ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥
                task.mark_failed(format!("{:?}", failure_reason), Some(failure_reason.clone()));
                storage_manager.update_message_status(task.id, crate::storage::entities::MessageStatus::Failed as i32).await?;
                
                error!("Worker {} final failure for {}: max retries exceeded", 
                      worker_id, task.id);
                
                // é€šçŸ¥å‘é€çŠ¶æ€ï¼šFailed
                let timestamp = SystemTime::now().duration_since(UNIX_EPOCH).unwrap().as_secs();
                event_manager.emit(SDKEvent::SendStatusUpdate {
                    channel_id: task.message_data.channel_id,
                    id: task.id as u64,
                    state: SendStatusState::Failed,
                    attempts: task.retry_count,
                    error: Some(format!("{:?}", failure_reason)),
                    timestamp,
                }).await;
            }
        }

        Ok(())
    }

    /// è·å–é¢‘é“é”
    async fn get_channel_lock(
        channel_locks: &Arc<RwLock<HashMap<ChannelKey, Arc<Mutex<()>>>>>,
        channel_key: ChannelKey,
    ) -> Arc<Mutex<()>> {
        // å…ˆå°è¯•è¯»é”
        {
            let locks = channel_locks.read().await;
            if let Some(lock) = locks.get(&channel_key) {
                return lock.clone();
            }
        }

        // éœ€è¦åˆ›å»ºæ–°é”ï¼Œè·å–å†™é”
        let mut locks = channel_locks.write().await;
        locks.entry(channel_key).or_insert_with(|| Arc::new(Mutex::new(()))).clone()
    }

    /// å¯åŠ¨å»¶è¿Ÿé˜Ÿåˆ—å¤„ç†å™¨
    async fn start_delay_queue_processor(&self) {
        let delay_queue = self.delay_queue.clone();
        let task_queue = self.task_queue.clone();
        let shutdown_signal = self.shutdown_signal.clone();
        let is_running = self.is_running.clone();

        tokio::spawn(async move {
            info!("Delay queue processor started");

            loop {
                select! {
                    _ = shutdown_signal.notified() => {
                        info!("Delay queue processor received shutdown signal");
                        break;
                    }
                    _ = tokio::time::sleep(Duration::from_millis(100)) => {
                        if !*is_running.read().await {
                            break;
                        }

                        // æ£€æŸ¥å»¶è¿Ÿé˜Ÿåˆ—ä¸­æ˜¯å¦æœ‰åˆ°æœŸçš„ä»»åŠ¡
                        let mut expired_tasks = Vec::new();
                        {
                            let mut queue = delay_queue.lock().await;
                            while let Some(expired_item) = queue.next().await {
                                expired_tasks.push(expired_item.into_inner());
                            }
                        }

                        // å¤„ç†åˆ°æœŸçš„ä»»åŠ¡
                        for delayed_task in expired_tasks {
                            info!("Retry time reached for task: {}", delayed_task.task.id);
                            
                            // é‡æ–°å…¥é˜Ÿ
                            match task_queue.enqueue(delayed_task.task).await {
                                Ok(_) => {
                                    debug!("Successfully re-enqueued retry task");
                                }
                                Err(e) => {
                                    error!("Failed to re-enqueue retry task: {}", e);
                                }
                            }
                        }
                    }
                }
            }

            info!("Delay queue processor stopped");
        });
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub async fn get_metrics(&self) -> SendMetrics {
        self.metrics.read().await.clone()
    }

    /// æ¸…é™¤ç»Ÿè®¡ä¿¡æ¯
    pub async fn clear_metrics(&self) {
        let mut metrics = self.metrics.write().await;
        *metrics = SendMetrics::default();
    }

    /// æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿è¡Œ
    pub async fn is_running(&self) -> bool {
        *self.is_running.read().await
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::storage::queue::{MemoryTaskQueue, TaskQueueTrait};
    use crate::storage::queue::retry_policy::RetryPolicy;
    use crate::network::DummyNetworkStatusListener;
    use crate::events::EventManager;
    use tempfile::TempDir;

    async fn create_test_consumer() -> (SendConsumerRunner, TempDir) {
        let temp_dir = TempDir::new().unwrap();
        let base_path = temp_dir.path();

        let storage_manager = StorageManager::new_simple(base_path).await.unwrap();
        storage_manager.init_user("test_uid").await.unwrap();
        storage_manager.switch_user("test_uid").await.unwrap();

        let task_queue = Arc::new(MemoryTaskQueue::new());
        let client = Arc::new(tokio::sync::RwLock::new(None));
        let network_listener = Arc::new(DummyNetworkStatusListener::default());
        let network_monitor = Arc::new(NetworkMonitor::new(network_listener));
        let retry_manager = Arc::new(RetryManager::new(RetryPolicy::default()));
        let message_rate_limiter = Arc::new(MessageRateLimiter::new(
            crate::rate_limiter::MessageRateLimiterConfig::default(),
        ));
        let event_manager = Arc::new(EventManager::new(100));

        let consumer = SendConsumerRunner::new(
            SendConsumerConfig::default(),
            task_queue,
            Arc::new(storage_manager),
            client,
            network_monitor,
            retry_manager,
            message_rate_limiter,
            event_manager,
        );

        (consumer, temp_dir)
    }

    #[tokio::test]
    async fn test_consumer_lifecycle() {
        let (consumer, _temp_dir) = create_test_consumer().await;
        
        // å¯åŠ¨æ¶ˆè´¹è€…
        consumer.start().await.unwrap();
        assert!(consumer.is_running().await);
        
        // åœæ­¢æ¶ˆè´¹è€…
        consumer.stop().await.unwrap();
        assert!(!consumer.is_running().await);
    }

    #[tokio::test]
    async fn test_channel_key() {
        let key1 = ChannelKey::new(1001, 1);
        let key2 = ChannelKey::new(1001, 1);
        let key3 = ChannelKey::new(1002, 1);
        
        assert_eq!(key1, key2);
        assert_ne!(key1, key3);
    }

    #[tokio::test]
    async fn test_send_metrics() {
        let mut metrics = SendMetrics::default();
        
        metrics.send_attempt_total = 10;
        metrics.send_success_total = 8;
        metrics.retry_count_total = 15;
        
        metrics.update_retry_average();
        
        assert_eq!(metrics.success_rate(), 0.8);
        assert_eq!(metrics.average_retry_count, 1.5);
    }
} 